\section{Overview}
\label{sec:overview}

%\pmcomment{Should we include an example for what our work can do? This is how a few of the past papers begin their overview.}

\subsection{Fairness Criteria}

Decision making/scoring functions aim to optimize for a specific metric such as accuracy, precision, F1 score etc. 
Fairness criteria, on the other hand, quantify the relationship between the outcome metric across multiple subgroups or similar individuals among the population. 
Formal definitions of fairness focus on observational criteria, i.e., those that can be written down as a probability statement involving the joint distribution of the features, sensitive attributes, decision making function, and actual outcome. %from https://fairmlclass.github.io/4.html#/2
For example, a decision making function that selects amongst applicants to hire for a specific position may have different hiring rates for majority and minority population groups. 
Suppose $r$ denotes the return value of the decision function, and $s$ is an indicator denoting whether a candidate belongs to a minority population.
An example fairness criterion is the 80\%-rule~\citep{zafar2017fairness} states that
\begin{align*}
    \min\left\{\frac{\Pr[r|s]}{\Pr[r|\neg s]},  \frac{\Pr[r|\neg s]}{\Pr[r| s]} \right\} \geq 0.8 
\end{align*}
In our work, we focus on fairness criteria that can be expressed using Bernoulli random variables.
This covers a majority of popular fairness criteria~\citep{verma2018fairness}.
%\pmcomment{Discussion of Individual vs Group fairness here?}
Section~\ref{sec:casestudy} contains descriptive analyses and case studies of real world applications of \AVOIRmethodname{} on commonly used fairness criteria.
%\pmcomment{more about fairness criteria group fairness etc?}

\subsection{Fairness Specifications}
We aim to derive statistical guarantees about fairness criteria based on observed outputs. 
These guarantees are derived in the form of assertions over specifications.
For a given specification $\psi$, we denote the claim that $P[\psi = F] \geq 1 - \delta$ as $\psi: (F, \delta)$, where $\delta$ denotes the failure probability of a guarantee. 
Similar assertions can be made about observed and tracked variables. 
Specifically, let $X$ be an observed Bernoulli r.v\footnote{random variable}
\begin{equation}\label{eqn:eps-delta-defn}
  \phi_X \eqdef X: (\BarE[X], \epsilon, \delta) \equiv \Pr[|\E[X] - \BarE[X]| \geq \epsilon] \leq \delta  
\end{equation}
where $\BarE[X]$ denotes an empirical estimate of $E[X]$.
Given a stream of (observations, outcomes from the decision functions), and a specified threshold probability $\delta$, \AVOIRmethodname{} will continue to refine the estimate for a given specification until reaching the failure threshold.
%The method will terminate when the specification can be asserted as being either True or False with a likelihood $> 1 - \delta$.
%\pmcomment{We can use $\HatMu_X$ instead of $\BarE[X]$ - does it look cleaner?}

\subsection{Contributions}

%\pmcomment{Working on merging contributions sections.}
%\pmcomment{Grammar improvements commented out.}

\begin{comment}
\subsubsection{Improved Grammar}
Suppose $r$ is the return value of a decision function \texttt{f} that takes features \texttt{x} and minority class indicator \texttt{s} as input, the Python expression for the first half of the 80\%-rule as defined by \citet{albarghouthi2019fairness} would be

\begin{lstlisting}[columns=flexible, language=Python]
@spec( (E(r&(s=1))/E(s=1)) * (E(s=0)/E(r&(s=0)) ) >= 0.8)
\end{lstlisting}

Our improved grammar introduces the \texttt{given} keyword that helps simplify this expression.
In our grammar, this specification would be expressed as
\begin{lstlisting}[columns=flexible, language=Python]
@spec( E(r,given=(s=1)) / E(r,given=(s=0)) >= 0.8) 
\end{lstlisting}
\end{comment}



\begin{figure}[b]
    \centering
    \begin{subfigure}[b]{0.47\linewidth}
        \resizebox{\linewidth}{!}{
            \centering
            \input{avoir/images/bernoulli-n-comparison-pgf-tikz}
        }
        \caption{At the same concentration $\epsilon$, lower failure probability $\delta$ for the majority class.}
        \label{fig:n-comparison-hoeffding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
           \input{avoir/images/grammar}
         \caption{ $\langle E \rangle$ refers to pure expressions and $\langle comp-op \rangle$ is any arbitrary comparison operator $\in \{>, <, =, \neq\}$.}
         \label{fig:grammar}
    \end{subfigure}
    \caption{\figleft{} Failure probability of Bernoulli r.v. being concentrated around its mean for different $n$. H = (online) Hoeffding, AH = Adaptive Hoeffding. \figright{} Modified Grammar for Specification.}
\end{figure}

\subsubsection{Probabilistic Guarantees: Optimization for Concentration Bounds}
\label{sec:contributions:optimization}
 %Group fairness metrics typically aim to quantify relationships between different subgroups in the population. 

When monitoring the value of a fairness metric in real-world scenarios, a system makes sequential decisions based on observed outcomes.
Previous work on verifying fairness improve bounds using better concentration from union bounding Hoeffding inequality~\citep{albarghouthi2017fairsquare,albarghouthi2019fairness} to the Adaptive Hoeffding inequality~\citep{zhao2016adaptive} used by \cite{bastani2019probabilistic}) to reduce the number of samples required to generate low failure probability estimates for a specification.
The overall failure probability of an assertion is computed as the sum of failure probabilities of each constituting sub-expression (using the union bound).
Proving guarantees for overall uncertainty across multiple groups involves balancing it across groups with differences in the number of observed samples.
We derive a method for computing improved probabilistic bounds on point estimates of a model's adherence to a specification,
%providing users with confidence about their model's behavior with respect to a specification 
in \emph{provably fewer} iterations in the \emph{online setting}.
Prior work does not adequately optimize the union-bounding procedure, leading to weaker bounds. 
%\pmcomment{need an alternative phrase to `weaker'/`stronger' below}.
Specifically, they allocate equal failure likelihoods among all sub-expressions of a given expression.
This is inefficient as fairness criteria typically involve estimating sub-expressions for terms having different numbers of observations (minority and majority groups).
Such an estimator can tolerate a higher failure probability with lower change in the confidence interval. 
%\pmcomment{metric vs criteria for fairness?}
For example, consider Bernoulli r.vs  $X_{1,2}$ for which we derive concentration guarantees $\Pr[|\E[X_i] - \BarE[X_i]| \geq \epsilon_{i}] \leq \delta_{i}$ after $t_{i}$ observations. 
%As used in our work, $X_{1,2}$ are $1/2$-subgaussian~\cite{zhao2016adaptive} and thus, $\epsilon, \delta$ are related via a pointwise Hoeffding inequality, i.e., 
From the Hoeffding inequality, $\delta = 2e^{-2t\epsilon^2}$.
We can claim stronger guarantees for $X_2$ if $t_2 > t_1$ as the failure probability is lower at the same concentration. 
This observation enables us optimize over sub-expressions to provide stronger overall concentration for compound expressions.
%Figure~\ref{fig:n-comparison-hoeffding} demonstrates this for $n = \left<200, 1000\right>$.
Adaptive versions of these inequalities also have similar patterns (see Figure~\ref{fig:n-comparison-hoeffding}).
%Thus, we can provide stronger overall guarantees for compound expressions (e.g. $\E[X_1] / \E[X_2]$) by allowing more margin for expressions having smaller number of observations $n$. 
Section~\ref{sec:theoretical} provides further details on how this allocation can be optimized.
%\pmcomment{Can potentially prove a theorem about the properties of convergence bounds having this property in general.}


\subsubsection{Inference Engine for Arbitrary Metrics}
%\pmcomment{Add some details of the visualizaiton here, and move the rest to section 3. + from older drafts.}
One mechanism to support monitoring is to construct a custom domain specific language (DSL, see Figure~\ref{fig:grammar}) parser and perform edit operations on the extracted syntax tree.
% from a parsed specification.
Our implementation takes a different route - we implemented the entire DSL using python. 
This provides an easy mechanism to integrate \AVOIRmethodname{} into existing python ML pipelines.
%\pmcomment{Since you provided options - why is the second option better or preferred - make this clear}
%For demonstrating its use, 
We build our framework as a library for specifying fairness criteria as decorators over python functions. 
That is, each non-terminal expressible in the grammar corresponds to a unique python object. 
We construct classes to represent each of these objects and overload the appropriate operators such that any statement in the python version of the DSL corresponds to a unique statement in the modified grammar. 
The classes themselves contain code to carry out the inference necessary to compute the failure probabilities and optimization setup required for any fairness metric implemented in the grammar.
Assuming that some concentration bound on the sum/mean of the decision functions is available, \AVOIRmethodname{} supports tracking for any group fairness metric. 
%with assumptions derived from the concentration bounds used. 
We demonstrate \AVOIRmethodname{} using the Adaptive Hoeffding bound~\citep{zhao2016adaptive} which applies to sub-gaussian random variables.
    %that can can be expressed using sub-gaussian components. 
Further, we will open-source our implementation.
%that automatically performs all necessary internal inference to carry out this optimmization. 
Prior work either does not have open-source implementations available~\citep{albarghouthi2019fairness} or the available implementations only provide examples for specific metrics~\citep{bastani2019probabilistic} and require tedious manual optimization and implementation efforts to generalize. %do not generalize. 
    %\item \emph{Blackbox decision functions}. The guarantees derived for implementation of our approach do not assume any specific functional form for the decision function. This facilitates their use in both database and machine learning contexts (Section~\ref{sec:casestudy}).
    %\item \emph{A more expressive grammar for fairness specifications}. We extend the grammar defined by Fairness Aware Programming in order to be more usable, succinct, and expressive.
    
    \subsubsection{Visualization as a Tool for interactive refinement}

%Algorithmic fairness is usually quantified using fairness criteria.
%However, seminal prior work~\cite{kleinberg2017inherent} determined that except in highly constrained cases, it is impossible to simultaneously satisfy three popular fairness measures (calibration within groups, balance for positive class, and balance for negative class). 
%Some notions of fairness can only be achieved when traded-off against maximum accuracy~\cite{menon2018cost,han2019inherent}.
%In addition, 
There are a plethora of fairness criteria and subtle changes in their definition can change the implications on decision making~\citep{castelnovo2021zoo}.
Thus, practitioners need support when selecting, designing, and guaranteeing fairness for deployed machine learning algorithms.
We develop an application for visually analyzing adherence to a specification for any data/model. 
The application uses a novel visualization to allow users to understand how specification violations arise by using the context from surrounding violations.
%We use visualization is a tool for refinement.

