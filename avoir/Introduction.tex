\section{Introduction}

Advanced analytics and artificial intelligence (AI), along with its many benefits, pose significant threats to individuals and the broader society.
\cite{Hirsch20} identify invasion of privacy; manipulation of vulnerabilities;  bias against protected classes; increased power imbalances;  error; opacity and procedural unfairness; displacement of labor;  pressure to conform, and intentional and harmful use as some of the key areas of concern.
A core part of the solution to mitigate such risks is the need to make organizations accountable and ensure that the data they leverage and the models they build and use are both inclusive of marginalized groups and resilient against societal bias.
Deployed AI and analytic systems are complex multi-step processes that can incorporate several sources of risk at each step.
At each of these stages, determining accountability in the decision-making of AI processes requires a determination of who is accountable, for what, to whom, and under what circumstances \citep{nissenbaum1996accountability,cooper2022accountability}. 
A more comprehensive overview of the mechanisms that can support accountability for the different stages of machine learning system design can be found in the work of \citet{cooper2022accountability}.
Our analysis centers on auditing fairness claims of mathematical guarantees associated with automated, black-box decision-making processes.
Governments worldwide are wrestling with different implementations of auditing regulations and practices to increase the accountability of decision processes.
Recent examples include the New York City auditing requirements for AI hiring tools~\citep{vanderford2022nybiaslaw}, European data regulation (GDPR~\citep{GDPR}), accountability bills~\citep{congress2023hr,algtransparency2022} and judicial reports~\citep{justice2018free}.
These societal forces have led to the emergence of checklists~\citep{mitchell2019model,sokol2020explainability}, metrics of fairness~\citep{verma2018fairness}, and recently, algorithms and systems that observe and audit the behavior of AI algorithms.
Such ideas date back to the 1950s~\citep{Moore1956}.
However, research has been sporadic until very recently, with the widespread use of AI-based decision-making giving rise to the vision of algorithmic auditing~\citep{Clavell20}.
In this work, we present a framework called \AVOIRmethodname{}\footnote{AVOIR in French means ``to have'', and this acronym reflects both our aspirational goal to achieve fairness in advanced analytics and AI but also reflects what is currently verifiable given a dataset, a model, and a fairness specification.}, for auditing and verifying fairness online.
%We present a framework for {\it Auditing and Verifying fairness Online through Iterative Refinement}~(\AVOIRmethodname{})
%
\AVOIRmethodname{} builds upon the ideas on distributional probabilistic fairness guarantees~\citep{albarghouthi2019fairness,bastani2019probabilistic}, generalizing them to real-world data. 
%Code for reproducing this work is available at \href{https://github.com/pranavmaneriker/AVOIR}{https://github.com/pranavmaneriker/AVOIR}.

