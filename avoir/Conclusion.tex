\section{Conclusion}

We presented the \AVOIRmethodname{} framework to easily define and monitor fairness specifications online and aid in the refinement of specifications. \AVOIRmethodname{} is easy to integrate within modern database systems but can also serve as a standalone system evaluating whether black-box machine learning models meet specific fairness criteria on specific datasets (including both structured and unstructured data) as described in our case studies.
\AVOIRmethodname{} extends the grammar from Fairness Aware Programming~\cite{albarghouthi2019fairness} with operations that enhance expressiveness.
In addition, we derive probabilistic guarantees that improve the confidence with which specification violations are reported.
Through case studies, we demonstrate that \AVOIRmethodname{} can provide users with insights and context that contribute directly to refinement decisions.
To understand the robustness of \AVOIRmethodname{}, we evaluated it along two dimensions: the data/ML model used and changing parameters (thresholds, fairness definitions).
We demonstrated the robustness of the data/model used by evaluating three datasets of varying domains and types (criminal justice - COMPAS, text classification - RateMyProfs, census data - Adult Income). 
For robustness to the thresholds, we used varying failure probability levels ($0.05$, $0.1$, $0.15$) in our case studies.
Note that any probability thresholds over these values for the corresponding studies would converge in fewer iterations, while lower thresholds would require additional data samples.
Our framework builds the foundation for further improvements in fairness specification, auditing, and verification workflows.
Although contextual information from \AVOIRmethodname{} makes decisions more straightforward, it is not always clear how to alter a specification in light of a violation and its relevant context.

To assist in these decisions, we are currently examining mechanisms that suggest edits that are likely to achieve the desired intent of a model developer.
We plan to extend this work to provide intelligent specification refinement suggestions and support distributed machine learning settings.
In addition to improving the usability of our tools for making fairness specification refinements, we also envision a more scalable framework.
Our case studies looked at a single model with respect to a single dataset. 
However, real-world deployment of machine learning often contains many clients with models and datasets that may evolve and drift over time.
We also expect to examine efficient monitoring of machine learning behavior for a fairness specification in a distributed context, enabling horizontal scalability.
We believe techniques such as decoupling the observation of data and reporting results from monitoring the results are promising and can lead to the desired scalability.
%We also plan to explore the use of \AVOIRmethodname{} for fair ranking problems and tailored database integration.
