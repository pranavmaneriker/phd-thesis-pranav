\section{Related Work}
\label{sec:related}

There are a plethora of fairness criteria, and subtle changes in their definition can change the implications on decision-making~\cite{castelnovo2021zoo}.
Practitioners need support when selecting, designing, and guaranteeing fairness for deployed machine learning algorithms.
Prior work on fairness has helped develop nuanced notions and algorithms to help train more `fair' machine learning models.
These include group fairness measures such as inter alia, minimizing disparate impact~\citep{calders2009building,feldman2015certifying}, maximizing the equality of opportunity~\citep{hardt2016equality}
In contrast with group fairness notions, causal notions of fairness~\cite{kusner2017counterfactual} and individualized notions of fairness~\cite{dwork2012fairness} provide alternative statistical mechanisms for understanding discriminatory behaviors of automated decision systems.
\citet{thomas2019preventing} proposed the Seldonian Framework as a generic mechanism for model users to design algorithms that help train machine learning models that can regulate them against undesirable behaviors.
\citet{yan2022active} propose a query-efficient framework to audit an unknown function chosen from a known hypothesis class of decision-making functions.

We focus on the problem of detecting and diagnosing whether systems designed under any framework follow any prescribed regulatory constraints supported within the grammar of \AVOIRmethodname{}.
That is, we are agnostic to the framework; instead, we are interested in testing the adherence of models to specified criteria.
We use a probabilistic framework to verify this behavior.
Alternative frameworks such as the AI Fairness 360~\citep{bellamy2019AI}  provide mechanisms to quantify fairness uncertainty, though they are restricted to pre-supported metrics.
Uncertainty quantification~\citep{ghosh2021uncertainty,ginart2022mldemon} is an alternative mechanism to provide adaptive guarantees. 
However, existing work is designed for commonly used outcome metrics, such as accuracy and F1-score, rather than for fairness metrics. 
Justicia~\citep{ghosh2021justicia} optimizes uncertainty for fairness metrics estimates using stochastic SAT solvers but can only be applied to a limited class of tree-based classification algorithms.

Machine learning testing~\citep{mltesting} is an avenue that can expose undesired behavior and improve the trustworthiness of machine learning systems.
Prior work on fairness testing is most closely related to \AVOIRmethodname{}.
Fairness testing~\citep{galhotra2017fairness} provides a notion of causal fairness and generates tests to check the fairness of a given decision-making procedure.
Given a specific definition of fairness, Fairtest~\citep{fairtest} and Verifair (VF)~\citep{bastani2019probabilistic} build a comprehensive framework for investigating fairness in data-driven pipelines. 
%By auditing the system's fairness, a more balanced rule can be generated for future use.
Fairness-aware Programming (FP) \citep{albarghouthi2019fairness} combined the two demands of machine learning testing and fairness auditing to make fairness a first-class concern in programming. 
Fairness-aware programming applies a runtime monitoring system for a decision-making procedure with respect to an initially stated fairness specification.
The overall failure probability of an assertion is computed as the sum of the failure probabilities of each constituting sub-expression (using the union bound).
FP does not provide any specific mechanism for splitting uncertainty, and Verifair splits it equally across all constituent \textit{elementary subexpressions}.
Thus, assertion bounds for subexpressions in both FP and VF are split inefficiently compared to \AVOIRmethodname{}. 
