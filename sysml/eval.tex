\label{sec:sysml:eval}
While ground truth labels for a single author having multiple accounts are unavailable, individual models can still be compared by measuring their performance on authorship attribution as a proxy. 
%We compare these approaches using a retrieval based setup over the generated output embeddings which reflects the clustering of episodes for each author.
We evaluated our method using retrieval-based metrics over the embeddings generated by each approach. % where embeddings corresponding to each approach are generated, and a random subset of the episode embeddings is sampled.
Denote the set of all episode embeddings as $E = \{e_1, \dots e_n\}$ and let $Q = \{q_1, q_2, \dots q_\kappa\} \subset E$ be the sampled subset.
We computed the cosine similarity of the query episode embeddings with all episodes. Let $R_i = \langle r_{i1}, r_{i2}, \dots r_{in} \rangle$ denote the list of episodes in $E$ ordered by their cosine similarity with episode $q_i$ (excluding itself) and let $A(.)$ map an episode to its author. The following measures are computed.

\noindent \textbf{Mean Reciprocal Rank}: (MRR) The RR for an episode is the reciprocal rank of the first element (by similarity) with the same author. MRR is the mean of reciprocal ranks for a sample of episodes.
\begin{align*}
    MRR(Q) = \frac{1}{\kappa}\sum_{i=1}^\kappa \frac{1}{\min\limits_j  \left(A(r_{ij}) = A(e_i)\right)}
\end{align*}
%where $\tau_i = \min\limits_j  \left(A(r_{ij}) = A(e_i)\right)$ is the rank of the nearest episode by the same author.

\noindent \textbf{Recall@k}:  (R@k)  Following~\citet{andrews2019learning}, we define the R@k for an episode $e_i$ to be an indicator denoting whether an episode by the same author occurs within the subset $ \langle r_{i1}, \dots, r_{ik} \rangle$. R@k denotes the mean of these recall values over all the query samples.

\begin{align*}
    R@k = \frac{1}{\kappa}\sum\limits_{i=1}^{\kappa}{\1_{\{\exists\ j | 1 \leq j \leq k, A(r_{ij}) = A(e_i)\}}}
\end{align*}

\noindent \textbf{Baselines}
We compare our best model against two baselines. First, we consider a popular short text authorship attribution model~\cite{shrestha2017convolutional} based on embedding each post using character CNNs. 
While the method had no support for additional attributes (time, context) and only considers a single post at a time, we compare variants that incorporate these features as well. 
The second method for comparison is invariant representation of users~\cite{andrews2019learning}. This method considers only one dataset at a time and does not account for graph-based context information. 
Results for episodes of length 5 are shown in Table~\ref{tab:baselines_comparison}

%\noindent \textbf{Hyperparameters}

\begin{table*}
    \centering
    \resizebox*{\linewidth}{!}{
		\begin{tabular}{lcccccccc}
 		\toprule
			\multirow{2}{*}{Method}	&\multicolumn{2}{c}{BMR}	&	\multicolumn{2}{c}{Agora}	&	\multicolumn{2}{c}{SR2}	&	\multicolumn{2}{c}{SR}\\
					&MRR&	R@10&	MRR&	R@10&	MRR&	R@10&	MRR&	R@10\\
		\midrule
			\citet{shrestha2017convolutional} (CNN) &	0.07	&	0.165	&	0.126	&	0.214	&	0.082	&	0.131	&	0.036	&	0.073	\\
			 + time + context &	0.235	&	0.413	&	0.152	&	0.263	&	0.118	&	0.21	&	0.094	&	0.178	\\
			 + time + context + transformer pooling &	0.219	&	0.409	&	0.146	&	0.266	&	0.117	&	0.207	&	0.113	&	0.205	\\
		\hline
			\citet{andrews2019learning} (IUR) \\
			 mean pooling &	0.223	&	0.408	&	0.114	&	0.218	&	0.126	&	0.223	&	0.109	&	0.19	\\
			 transformer pooling&	0.283	&	0.477	&	0.127	&	0.234	&	\textit{0.13}	&	\textit{0.229}	&	0.118	&	0.204	\\
		\hline
			\SYSMLmethodname{} (single) &	\textit{0.32}	&	\textit{0.533}	&	\textit{0.152}	&	\textit{0.279}	&	0.123	&	0.21	&	\textit{0.157}	&	\textit{0.266}	\\
			 - graph context &	0.265	&	0.454	&	0.144	&	0.251	&	0.089	&	0.15	&	0.049	&	0.094	\\
			 -graph context - time &	0.277	&	0.477	&	0.123	&	0.198	&	0.079	&	0.131	&	0.04	&	0.08	\\
		\hline
			\SYSMLmethodname{}  (multitask) &	\textbf{0.438}	&	\textbf{0.642}	&	\textbf{0.303}	&	\textbf{0.466}	&	\textbf{0.304}	&	\textbf{0.464}	&	\textbf{0.227}	&	\textbf{0.363}	\\
			 - graph context &	0.396	&	0.602	&	\textbf{0.308}	&	\textbf{0.469}	&	0.293	&	0.442	&	0.214	&	0.347	\\
			 - graph context - time &	0.366	&	0.575	&	0.251	&	0.364	&	0.236	&	0.358	&	0.167	&	0.28	\\
		\bottomrule
	\end{tabular}
    }
    \caption{Best performing results in {\bf bold}. Best performing single-task results in {\it italics}. All  $\sigma_{MRR}< 0.02$, $\sigma_{R@10} < 0.03$, For all metrics, higher is better. Results suggest single-task performance largely outperforms the state-of-the-art \cite{shrestha2017convolutional,andrews2019learning},
    while our novel multi-task cross-market setup offers a substantive lift ({\bf up to 2.5X on MRR and 2X on R@10}) over single-task performance.}
    \label{tab:baselines_comparison}
\end{table*}



