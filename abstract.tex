
The success of neural networks and the advent of specialized hardware such as GPUs has led to larger models with increasingly large unstructured datasets in machine learning.
Curating and assembling a high-quality large dataset is a time-consuming and expensive process.
Further, training models on these datasets requires expensive computing resources.
Some of these issues are alleviated with the advent of paradigms such as self-supervised and transfer learning.
However, when the data continue to drift and change over time,
%, these paradigms are unable to adapt; 
models need to be periodically retrained to keep up.

Graph structures, both implicit and explicit, are ubiquitous in Natural Language Processing.
Implicit structures are derived from language morphology, syntax, and semantics and expressed using attributed tree graphs.
External structures capture world knowledge and semantics using knowledge graphs and ontologies.
Additionally, textual data may have associated metadata in external graphs, such as network structure for social media interactions.
In this dissertation, we posit that an abundance of associated structural information is underutilized for scaling and adaptation.
The prevalence of these structures behooves us to utilize them to improvise, adapt, and overcome the challenges posed by scaling and drifts in data.

%I the role of graph structures in augmenting natural language processing and machine learning in order to make them adapt to changes in the underlying data over time. 
In our work, we focus on three broad directions in which to use these structures, viz. augmenting existing text models with structure, exploring the role of structure in creating adversarial testing samples, and structured-enhanced monitoring of the performance of models over time.
The first direction that we explore is the impact of incorporating structure into text representation learning pipelines.
In our first contribution, we study how the implicit structure of text data (here, URLs) can be used to design domain-specific losses and adversarial attacks to build a state-of-the-art system for phishing URL detection.
This work comprehensively analyzes transformer models on the phishing URL detection task.
We consider the standard masked language model and additional domain-specific pre-training tasks and compare these models to fine-tuned transformer models.
Our model improves over the best baseline over a range of low false positive rates.
We then demonstrate how these models can be more robust by using adversaries constructed from benign URLs using a domain-informed attack scenario. 
In both fine-tuning and adversarial attacks, the underlying syntax of URLs serves as the structure that enables us to build a robust model.


The second direction of work we study is the role of intrinsic structure in the visualization and analysis of the fairness of machine learning models.
Specifically, we study the syntax of commonly used fairness metrics 
Our contribution improves the probabilistic guarantees for such grammars in an interactive and online setting.
We construct a novel visualization mechanism that can be used to investigate the context of reported fairness violations and guide users toward meaningful and compliant fairness specifications.
Our framework requires certain assumptions about the data generating process at run time.
Following up to this work, we investigate techniques which can help expand probabilistic guarantees under weaker assumptions.
In particular, the setting of interest to us is one where dependencies between different data points are represented through a (predefined) network structure. We critically analyze the choices made and describe the trade offs associated with existing work in this domain.

Finally, in our third broad direction, we study the problem of author identification.
Our work demonstrates that it is possible to appropriately intermingle graph representation learning with textual representations to utilize the orthogonal signals from each and improve Author identification across time-disjoint task settings.
We first develop a novel stylometry-based multitask learning approach for natural language and model interactions using graph embeddings to construct low-dimensional representations of short episodes of user activity for authorship attribution. 
We comprehensively evaluate our methods across four darkweb forums demonstrating their efficacy over the state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X on Recall@10.
Next, we focus on the textual component of the author identification models.
We demonstrate that it is possible to use models trained on large, clear web datasets to improve author identification on darkweb forums.
We conclude this direction with a study of the limitations of the text-based models to generalize across time and demographics.


%For our second direction of work, we study the task of extracting a structured representation of unstructured data in evolving scenarios. 

For the latter two directions, our work has potential extensions, which we discuss in a concluding chapter on future work.
We empirically demonstrate that structure can be used to improve the author identification even with large scale datasets.
We provide concrete architectural suggestions that may be used to train models that utilize both the structure and content of large datasets.
Secondly, we propose potential extensions of our ideas proposed in the work on fairness monitoring.
We expand on our work on our theoretical framework for conformal prediction on graphs to fairness in graph structured data.
%Our theory relies on a specific setting for graph machine learning, but we propose future work towards extending these ideas to study fairness in graph structured data in a more general setting.
%We focus on settings involving data drift inductive prediction on graphs.
Finally, based on the observed limitations on the robustness of author identification models, we propose extensions of ideas explored in our work on temporal robustness that may be used to provide bounds on the generalization capabilities of these models.

\endinput