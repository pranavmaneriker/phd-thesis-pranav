Neighborhood Adaptive Prediction Sets (NAPS) can construct predictive sets via Conformal Prediction under relaxed exchangeability (or non-exchangeability) assumptions~\cite{barber2023conformal}.
In the context of graphs, NAPS was initially implemented in the inductive setting \cite{clarkson2023distribution}.
However, it can be used in the transductive setting as well~\cite{zargarbashi23conformal}. 


NAPS in the transductive setting is based on APS where $s_i = A(\vx_i, y_i, u_i;\hat{\pi}_i)$, or $A(\vx_i, y_i;\hat{\pi}_i)$ (depending on whether the randomized version is used), is computed for each node in $\gD_{\text{calib}}$. 
Using these scores, a weighted quantile is computed to produce the score threshold for prediction sets (Equation~\ref{eq:NAPS:quantile})
Unlike APS, the quantile is defined by placing weighted point masses ($\delta$) at each score from the calibration set under consideration for quantile computation.
The point mass at $+\infty$ indicates that the score for test node $n+1$ is unknown (and unbounded due to non-exchangeability), and thus, a point mass at the maximum value ($+\infty$) is required.%\ascomment{I want to state the intuitive reason for the infinite point mass since it was not clearly stated in the NAPS paper.} 
\begin{align}
    \hat{q}^{\text{NAPS}}_{n+1} = \text{Quantile}\bigg(1-\alpha, \bigg[\sum_{i\in\gD_{\text{calib}}}\Tilde{w}_{i}\cdot \delta_{s_{i}}\bigg] + \Tilde{w}_{n+1}\cdot \delta_{+\infty}\bigg)
    \label{eq:NAPS:quantile}
\end{align}

For NAPS to produce viable prediction sets, the weights, $w_i\in [0,1]$, for nodes under consideration in the calibration set must be chosen in a data independent fashion, i.e., they cannot leverage the feature vectors associated with the calibration nodes~\citep{barber2023conformal}.
NAPS leverages the graph structure to assign these weights, assigning non-zero weights to nodes within a k-hop neighborhood $\gN_{n+1}^k$ of the test node $v_{n+1}$.
%Since exchangeability is not assumed, a weight function leveraging graph homophily can be used to produce weights, $w_i\in [0,1]$, for nodes in the calibration set \cite{barber2023conformal}. 
The three implemented weight functions are uniform, $w_u(d_i) = 1$, hyperbolic $w_h(d_i) = \frac{1}{d_i}$, and exponential, $w_e(d_i) = 2^{-d_i}$ for nodes in the k-hop neighborhood, where $d_i$ is the distance from $v_{n+1}$ to $v_i \in \gV_{\text{calib}}$.
Formally, the weight function for each node, $v_i \in \gV_{\text{calib}}$ can be seen in Equation \ref{eq:NAPS:weight} below, where $w_x(d_i)$ is the selected weight function.
These weights are then normalized to compute $\Tilde{w}_i$ such that $\sum_{i\in\gD_{\text{calib}}} \Tilde{w}_i + \Tilde{w}_{n+1} = 1$ \cite{barber2023conformal}.
\begin{align}
    w_i = \begin{cases}
w_x(d_i), & i\in \gD_{\text{calib}}\cap\mathcal{N}_{n+1}^k\\
0,& i\in \gD_{\text{calib}}\setminus\mathcal{N}_{n+1}^k
\end{cases}
    \label{eq:NAPS:weight}
\end{align}

Using the NAPS quantile, $\hat{q}^{\text{NAPS}}_{n+1}$, the prediction sets can be constructed similarly to other Conformal Prediction algorithms.
Note that NAPS was originally designed for the inductive setting; transductive differs as non-zero weights are assigned to fewer nodes as only a subset of the graph nodes are assigned to be calibration nodes.
In the inductive setting, no exchangeability cannot be assumed and the entire graph prior to the test phase is considered as a calibration set leading to a larger number of non-zero weights.

\noindent \textbf{NAPS Implementation}
NAPS is computationally more expensive with regard to time and memory as a k-hop intersection must be computed for each test node.
We optimized this implementation using a batched approach that works with sparse tensors (Algorithm \ref{alg:NAPS:Quantile}).
Informally, the test nodes are first split up into batches. 
Then, for each batch, the distance to each node in the k-hop neighborhood is computed.
Following this, the weights function for the corresponding nodes are computed before computing the quantile for each node.
The batched approach ensures that sufficient memory is available for the necessary computations - especially for computing the distance to each node in the k-hop neighborhood without needing to densify a sparse graph.

\begin{algorithm}
\caption{NAPS Quantile Implementation}\label{alg:NAPS:Quantile}
\begin{algorithmic}[1]
\Procedure{NAPS\_Quantile}{$w,k,\calib,\test,\mathcal{D},\mathcal{S}_{\text{calib}},b,\alpha$}
    \State $\{\mathcal{B}_1,\mathcal{B}_2,\hdots,\mathcal{B}_b\}\gets \Call{\text{split}}{\test,b}$ \Comment{Split test nodes into b batches}
    \State $q\gets \Call{\text{zeros}}{\test,1}$\Comment{$q\in \mathbb{R}^{|\test| \times 1}$}
    \For{$\mathcal{B}_n \in \{\mathcal{B}_1,\mathcal{B}_2,\hdots,\mathcal{B}_b\}$}
        \State $\text{k\_hop} \gets \Call{\text{Sparse\_k\_hop}}{k,\mathcal{B}_n,\calib,\mathcal{D}}$\Comment{k\_hop $\in \mathbb{R}^{|\mathcal{B}_n|\times|\calib|}$}
        \State $\text{weights}\gets \Call{\text{compute\_weights}}{w,\text{k\_hop}}$\Comment{weights $\in \mathbb{R}^{|\mathcal{B}_n|\times|\calib|}$}
        \State $q[\mathcal{B}_n]\gets \Call{\text{compute\_quantile}}{1-\alpha,\text{weights},\mathcal{S}_{\text{calib}}}$
    \EndFor\label{NAPSquantileendwhile}
    \State \textbf{return} $q$\Comment{Return the quantiles for each test node}
\EndProcedure
\end{algorithmic}
\end{algorithm}

To ensure scalability for large graphs, all the computations until the quantile computation setep were done via sparse tensors. 
Algorithm \ref{alg:NAPS:SparseKHop} illustrates how the distance to each calibration node in the k-hop neighborhood can be computed via  sparse tensorr primitives.
The sign function based formulation uses the fact that subtracting $n+1$-hop paths from a matrix containing up to $n$ hops to ensure negative values at paths of length exactly $n+1$, with the rest being 0.
%To ensure that the minimum distance to nodes in the k-hop neighborhood is reported, the sign function is applied to the matrix containing paths exactly n hops away. This is subtracted from the matrix containing distances up to n-1 hops. A value can only be less than 0 after this subtraction if the corresponding index in the matrix containing distances up to n-1 hops was 0. Using this, the nodes that are at a minimum n hops away can be identified and added to the matrix containing distances to calibration nodes in the k-hop neighborhood. The described calculations can be done using sgn, and signbit (to check if an element is negative) in PyTorch.\ascomment{Shorted this paragraph}  

\begin{algorithm}
\caption{Sparse K Hop Neighborhood Implementation}\label{alg:NAPS:SparseKHop}
\begin{algorithmic}[1]
\Procedure{Sparse\_k\_hop}{$k,\mathcal{B},\calib,\mathcal{D}$}
    \State $\text{A}\gets \Call{\text{Get\_Adjacency}}{\mathcal{D}}$ \Comment{Adjacency of $\mathcal{D}$, A $\in \mathbb{R}^{|\mathcal{D}|\times|\mathcal{D}|}$}
    \State $\text{path\_n}\gets \text{A[$\mathcal{B},:$]}$\Comment{path\_n $\in \mathbb{R}^{|\mathcal{B}|\times|\mathcal{D}|}$ }
    \State $\text{k\_hop}\gets \text{path\_n[$:,\calib$]}$\Comment{k\_hop $\in \mathbb{R}^{|\mathcal{B}|\times|\calib|}$ }
    \For{$n \in \{2,3,\hdots,k\}$}
        \State $\text{path\_n} \gets (\text{path\_n})\text{A}$
        \State $\text{neg\_if\_n}\gets \text{k\_hop}-\Call{\text{sgn}}{\text{path\_n[$:,\calib$]}}$\Comment{negative value $\implies$ n hops away}
        \State $\text{in\_n\_hop}\gets (\text{neg\_if\_n}<0)\times n$ \Comment{Nodes that are a min distance of n}
        \State $\text{k\_hop} \gets \text{k\_hop} + \text{in\_n\_hop}$
    \EndFor\label{khopendwhile}
    \State \textbf{return} $\text{k\_hop}$\Comment{$\forall_{i,j}\, \text{\textbf{If} dist($i,j$)}  \leq k \text{ \textbf{then} k\_hop[$i,j$]} = \text{dist}(i,j), \text{\textbf{else} k\_hop[$i,j$]}=0$}
\EndProcedure
\end{algorithmic}
\end{algorithm}