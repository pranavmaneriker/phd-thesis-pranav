There are several methods of partitioning the data to generate the different partitions of the sets. 
Two methods which are used in other works on graph conformal prediction for classification are (1) full-split paritioning~\cite{huang2024uncertainty} and (2) label-based sample partitioning~\cite{zargarbashi23conformal}.
%\avcomment{Add citations for methods that use each one} %These methods originate from works that consider the classification task in either a supervised or semi-supervised setting, respectively.
 
\noindent \textbf{Full-Split (FS) Paritioning}
In this case, the choice of data split can be done such that each subset of the partition adheres to a size constraint defined in terms of a percentage/fraction of the full node set $\gV$.
For example, in CF-GNN \cite{huang2024uncertainty} the authors split the datasets in their experiments randomly, but adhering to a $20/10/35/35$ split of $\gD_{\text{train}}, \gD_{\text{valid}},$, $\gD_{\text{calib}}$ and  $\gD_{\text{test}}$.
Note that the overall percentage of data for which we do provide labels (in either the development or calibration set) is large ($65\%$).
For non-conformal scores models with a large number of trainable parameters, this splitting scheme is ideal as it allows for a large amount of data to be used for training the calibration score  model.
We explore the following splitting schemes under FS partitioning:
($\gD_{\text{train}}, \gD_{\text{valid}},$, $\gD_{\text{calib}}$, $\gD_{\text{test}}$) = ($20/10/35/35$), ($20/20/30/30$), ($30/10/30/30$), and ($30/20/25/25$).

\noindent \textbf{Label-Count (LC) Sample Paritioning}
In this splitting scheme, the data is split to ensure an equal number of samples for each class/label are present in the train, validation, and calibration set.
The remaining nodes are then used for the test set.
Such settings are common in  settings where only a small proportion of training/labeled nodes are available, such as in semi-supervised learning.
This setting is ideal for methods that do not have a large number of parameters to train.
We explore setting the number of samples per class to 10, 20, 40, and 80.
Note that we assign nodes for each class into train, validation, calibration, and test sets sequentially, so it is feasible in this setup to have some classes having no representative samples in some partitions. 