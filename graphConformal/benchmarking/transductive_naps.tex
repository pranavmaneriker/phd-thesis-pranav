The Neighborhood Adaptive Prediction Sets (NAPS) can construct predictive sets via Conformal Prediction under relaxed exchangeability (or non-exchangeability) assumptions \cite{barber2023NAPS}. In the context of graphs, NAPS was initially implemented in the inductive setting \cite{clarkson2023distribution}. However, it can be used in the transductive setting as well \cite{zargarbashi23conformal}. 


Transductive NAPS is based on APS where $s_i = A(\vx_i, y_i, u_i;\hat{\pi}_i)$, or $A(\vx_i, y_i;\hat{\pi}_i)$ if $\epsilon$ is not used, is computed for each node in $\gD_{\text{calib}}$. Using these scores, a weighted quantile is computed to produce a threshold for a class to be included in the prediction set as seen in Equation \ref{eq:NAPS:quantile} below by creating weighted point masses ($\delta$) at each score. The point mass at $+\infty$ indicates that the score for test node n+1 is unknown (and unbounded due to non-exchangeability), and thus, a point mass at the maximum value ($+\infty$) is required.\ascomment{I want to state the intuitive reason for the infinite point mass since it was not clearly stated in the NAPS paper.} 
\begin{align}
    \hat{q}^{\text{NAPS}}_{n+1} = \text{Quantile}\bigg(1-\alpha, \bigg[\sum_{i\in\gD_{\text{calib}}}\Tilde{w}_{i}\cdot \delta_{s_{i}}\bigg] + \Tilde{w}_{n+1}\cdot \delta_{+\infty}\bigg)
    \label{eq:NAPS:quantile}
\end{align}

Since exchangeability is not assumed, a weight function leveraging graph homophily can be used to produce weights, $w_i\in [0,1]$, for nodes in the calibration set \cite{barber2023NAPS}. The three implemented weight functions are uniform, $w_u(d_i) = 1$, hyperbolic $w_h(d_i) = \frac{1}{d_i}$, and exponential, $w_e(d_i) = 2^{-d_i}$ for nodes in the $k$ hop neighborhood of test node n+1, $\mathcal{N}_{n+1}^k$, where $d_i$ is the distance from the test node to node $i$ in the calibration set. Formally, the weight function - in the transductive setting - for each node, $i\in\gD_{\text{calib}}$ can be seen in Equation \ref{eq:NAPS:weight} below, where $w^x(d_i)$ is the selected weight function. These weights are then linearly normalized to compute $\Tilde{w}_i$ such that $1 = \sum_{i\in\gD_{\text{calib}}} \Tilde{w}_i + \Tilde{w}_{n+1}$ \cite{barber2023NAPS}.
\begin{align}
    w_i = \begin{cases}
w_x(d_i), & i\in \gD_{\text{calib}}\cap\mathcal{N}_{n+1}^k\\
0,& i\in \gD_{\text{calib}}\setminus\mathcal{N}_{n+1}^k
\end{cases}
    \label{eq:NAPS:weight}
\end{align}

Using the NAPS quantile, $\hat{q}^{\text{NAPS}}_{n+1}$, the prediction sets can be constructed similarly to other Conformal Prediction algorithms. Lastly, it should be noted that transductive and inductive NAPS differ from each other since transductive NAPS applies non-zero weights only to a subset of calibration nodes, while inductive NAPS uses the entire k-hop neighborhood of a test node.\ascomment{Verify this claim (i.e. reread the paper)}  

\subsubsection{A Word on Implementation}
NAPS is computationally expensive with regard to time and memory. To redress, we took a batched approach that worked with sparse tensors as detailed in Algorithm \ref{alg:NAPS:Quantile}. The test nodes are first split up into batches. Then, for each batch, the distance to each node in the k-hop neighborhood is computed and returned. Then, the corresponding weights are computed before computing each test node's quantile. The batched approach ensures that enough memory is available for the necessary computations - especially for computing the distance to each node in the k-hop neighborhood.  

\begin{algorithm}
\caption{NAPS Quantile Implementation}\label{alg:NAPS:Quantile}
\begin{algorithmic}[1]
\Procedure{NAPS\_Quantile}{$w,k,\calib,\test,\mathcal{D},\mathcal{S}_{\text{calib}},b,\alpha$}
    \State $\{\mathcal{B}_1,\mathcal{B}_2,\hdots,\mathcal{B}_b\}\gets \Call{\text{split}}{\test,b}$ \Comment{Split test nodes into b batches}
    \State $q\gets \Call{\text{zeros}}{\test,1}$\Comment{$q\in \mathbb{R}^{|\test| \times 1}$}
    \For{$\mathcal{B}_n \in \{\mathcal{B}_1,\mathcal{B}_2,\hdots,\mathcal{B}_b\}$}
        \State $\text{k\_hop} \gets \Call{\text{Sparse\_k\_hop}}{k,\mathcal{B}_n,\calib,\mathcal{D}}$\Comment{k\_hop $\in \mathbb{R}^{|\mathcal{B}_n|\times|\calib|}$}
        \State $\text{weights}\gets \Call{\text{compute\_weights}}{w,\text{k\_hop}}$\Comment{weights $\in \mathbb{R}^{|\mathcal{B}_n|\times|\calib|}$}
        \State $q[\mathcal{B}_n]\gets \Call{\text{compute\_quantile}}{1-\alpha,\text{weights},\mathcal{S}_{\text{calib}}}$
    \EndFor\label{NAPSquantileendwhile}
    \State \textbf{return} $q$\Comment{Return the quantiles for each test node}
\EndProcedure
\end{algorithmic}
\end{algorithm}

To ensure scalability for large graphs, all of the computations up until computing the quantile were done via sparse tensors. Algorithm \ref{alg:NAPS:SparseKHop} illustrates how the distance to each calibration node in the k-hop neighborhood can be computed via supported sparse tensor operations in PyTorch. To ensure that the minimum distance to nodes in the k-hop neighborhood is reported, the sign function is applied to the matrix containing paths exactly n hops away. This is subtracted from the matrix containing distances up to n-1 hops. A value can only be less than 0 after this subtraction if the corresponding index in the matrix containing distances up to n-1 hops was 0. Using this, the nodes that are at a minimum n hops away can be identified and added to the matrix containing distances to calibration nodes in the k-hop neighborhood. The described calculations can be done using sgn, and signbit (to check if an element is negative) in PyTorch.\ascomment{Shorted this paragraph}  

\begin{algorithm}
\caption{Sparse K Hop Neighborhood Implementation}\label{alg:NAPS:SparseKHop}
\begin{algorithmic}[1]
\Procedure{Sparse\_k\_hop}{$k,\mathcal{B},\calib,\mathcal{D}$}
    \State $\text{A}\gets \Call{\text{Get\_Adjacency}}{\mathcal{D}}$ \Comment{Adjacency of $\mathcal{D}$, A $\in \mathbb{R}^{|\mathcal{D}|\times|\mathcal{D}|}$}
    \State $\text{path\_n}\gets \text{A[$\mathcal{B},:$]}$\Comment{path\_n $\in \mathbb{R}^{|\mathcal{B}|\times|\mathcal{D}|}$ }
    \State $\text{k\_hop}\gets \text{path\_n[$:,\calib$]}$\Comment{k\_hop $\in \mathbb{R}^{|\mathcal{B}|\times|\calib|}$ }
    \For{$n \in \{2,3,\hdots,k\}$}
        \State $\text{path\_n} \gets (\text{path\_n})\text{A}$
        \State $\text{neg\_if\_n}\gets \text{k\_hop}-\Call{\text{sgn}}{\text{path\_n[$:,\calib$]}}$\Comment{negative value $\implies$ n hops away}
        \State $\text{in\_n\_hop}\gets (\text{neg\_if\_n}<0)\times n$ \Comment{Nodes that are a min distance of n}
        \State $\text{k\_hop} \gets \text{k\_hop} + \text{in\_n\_hop}$
    \EndFor\label{khopendwhile}
    \State \textbf{return} $\text{k\_hop}$\Comment{$\forall_{i,j}\, \text{\textbf{If} dist($i,j$)}  \leq k \text{ \textbf{then} k\_hop[$i,j$]} = \text{dist}(i,j), \text{\textbf{else} k\_hop[$i,j$]}=0$}
\EndProcedure
\end{algorithmic}
\end{algorithm}