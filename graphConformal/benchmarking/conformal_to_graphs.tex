The usual tasks of interest in graph data are node classification, link prediction, and graph classification. 
In this work, we focus on node classification and its extensions to conformal prediction.
Consider an attributed homogeneous graph $\gG = (\gV, \gE, \mX)$, where $\gV$ is the set of nodes, $\gE$ is the set of edges and $\mX$ is the set of node attributes.
Let $\mA$ denote the adjacency matrix for the graph.
Further, let $\gY = \{1, \dots, K\}$ denote set of class labels associated with the nodes.
For $v \in \gV$, $\vx_v \in \R^d$ denotes its features and $y_v \in \{1, \dots, K\}$ denotes the corresponding class label.
The task of node classification is to learn a model $F: \gX \to Y$ which predicts the label for each node given node features as input.
Corresponding to the CP partitions, we denote the nodes in the training set as $\gV_{\text{train}}$, validation set as $\gV_{\text{valid}}$, calibration set as $\gV_{\text{calib}}$, and test set as $\gV_{\text{test}}$.
We denote $\gV_d = \gV_{\text{train}} \cup \gV_{\text{valid}}$ 
as the development set of the base model (non-conformalized). 
Note that labels are available only for nodes in the train, validation and calibration sets, and must be predicted for the test set.
The model cycle will involve four phases, viz. training, validation, calibration, and testing.
Next, we discuss the different settings for node classification in graphs and the applicability of conformal prediction.

\noindent \textbf{Transductive setting}
In this setting, the model has access to the fixed graph $\gG$ during training, validation, calibration, and testing.
However, the labels associated with the test nodes $\gD_{\text{test}}$ are unknown. 
%We assume that $\gV_{\text{test}} \cap \gV_{\text{calib}}$ are exchangeable.
We designate a fixed set of nodes disjoint from the training and validation set as  $\gV_{\text{test}} \cup \gV_{\text{calib}}$ and then randomly sample nodes from this set to form $\gV_{\text{calib}}$ and $\gV_{\text{test}}$.
This is the setting considered in~\citet{zargarbashi23conformal} and~\citet{huang2024uncertainty}.
Note that the labels for the calibration nodes are not available for training/validation of the base model, though the neighborhood information $(\gV, \gE)$ and the features $\vx_v$ and labels $y_v$, $v \in \gV_d$ are available.
During the calibration phase, the features and labels for the calibration nodes, along with the neighborhood information, are used to compute the non-conformity scores.
This split ensures that the base model cannot distinguish between the calibration and test nodes, and hence exchangeability holds for $v \in \gV_{\text{calib}} \cup \gV_{\text{test}}$.

\noindent \textbf{Inductive setting}
We briefly describe the inductive setting and note that the exchangeability assumption will be violated in this setting in general.
The base model is provided with the graph induced by the development nodes only $(\gV_d, \gE_d, \mX_d)$.
In the calibration/test phases, the nodes arrive either one at a time or in batches.
Thus, nodes arriving later in the sequence will have access to neighbors that arrived earlier, breaking the exchangeability assumption.

In line with previous work, we focus on the transductive setting.
The following theorem shows that in the transductive setting, a score model trained on the calibration set will generate scores exchangeable with the test set, and thus allow the use of conformal prediction in the transductive setting.

\begin{theorem}[\citet{zargarbashi23conformal,huang2024uncertainty}]
    Let $\gG = (\gV, \gE, \mX)$ be an attributed graph, and $\gV_{\text{calib}} \cup \gV_{\text{test}}$ be exchangeable.
    Let $F: \gX^{|V|} \rightarrow \Delta^{|V| \times K}$ be any permutation equivariant model on the graph (for instance, GNN). 
    Define $F(G) = \Pi \in \Delta^{|V| \times K}$ be the output probability matrix for a model trained on only $\gV_d$.
    Then any score function $s(v, y) = s(\Pi_v, y, \gG)$ is exchangeable for all $v \in \gV_{\text{calib}} \cup\gV_{\text{test}} $
    %Let $F: \gX \to \Delta_{\gY}$ be a model trained on $\gV_{\text{train}} \cup \gV_{\text{valid}}$.
    %Let $\hat{F}: \gX \to \Delta_{\gY}$ be a model trained on $\gV_{\text{calib}}$.
    %Then, the scores $\hat{F}(\vx)$ are exchangeable with $F(\vx)$ for $\vx \in \gV_{\text{test}}$.
    \label{thm:exchangeability}
\end{theorem}
The intuition for this theorem is that if the output of the permutation equivariant function (e.g., GNN) $F$ does not depend on the order of the nodes in the graph (for e.g. GNN output depends on the neighbors, not the order of the nodes), then the outputs of the GNN will also be exchangeable.
The formal proof for this theorem is available in~\citet{zargarbashi23conformal,huang2024uncertainty}.
This theorem paves the way for using conformal prediction for transductive node classification in graphs.
%\pmcomment{TODO: thm and proof for all scores}

%\pmcomment{Exchangeability for transductive, and potential for inductive}

%\subsubsection{Node Classification}
%Let $f: \gX \to \R_K$ denote the class wise scores associated with a node classification model trained on a separate split of the data ($\gD_{\text{train}}$).
%For example, these could be the pre-final output layer of a graph neural network (either before or after softmax normalization).
%and a trained model with classwise prediction the goal is to learn a model $\pi: \gX \to \Delta_K$, where $\Delta_K$ is the $K$-dimensional probability simplex.
%
%\pmcomment{below text optionol}
For the following sections, we will assume that the base model $\hat{\pi}: \gX \to \Delta_{\gY}$, where $\Delta_{\gY}$ is the probability simplex over the elements of $\gY$, is learned using the training and validation sets $\train \cup \valid$. 
The calibration set $\calib$ is used to determine the $\hat{q}(\alpha)$ from Theorem \ref{thm:CP:coverage} and the test set $\test$ is the set for which we want to compute our prediction sets.
In general, the scores need not lie over a simplex; they can be in $\R^K$.
However, this greatly simplifies the exposition for the following sections and is the standard practice in prior work.

